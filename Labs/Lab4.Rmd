---
title: "Lab4"
author: "Heather Childers"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(skimr)
library(tidymodels)
library(caret)
library(corrplot)
```

## Lab 4: Fire and Tree Mortality

The database we'll be working with today includes 36066 observations of individual trees involved in prescribed fires and wildfires occurring over 35 years, from 1981 to 2016. It is a subset of a larger fire and tree mortality database from the US Forest Service (see data description for the full database here: [link](https://www.nature.com/articles/s41597-020-0522-7#Sec10)). Our goal today is to predict the likelihood of tree mortality after a fire.

### Data Exploration

Outcome variable: *yr1status* = tree status (0=alive, 1=dead) assessed one year post-fire.

Predictors: *YrFireName, Species, Genus_species, DBH_cm, CVS_percent, BCHM_m, BTL* (Information on these variables available in the database metadata ([link](https://www.fs.usda.gov/rds/archive/products/RDS-2020-0001-2/_metadata_RDS-2020-0001-2.html))).

```{r}
trees_dat<- read_csv(file = "https://raw.githubusercontent.com/MaRo406/eds-232-machine-learning/main/data/trees-dat.csv")
```

> Question 1: Recode all the predictors to a zero_based integer form

```{r}
set.seed(375)
#Create the recipe for the trees data
tree_recipe <- recipe(yr1status ~ ., data = trees_dat) %>% 
  step_integer(all_predictors(), zero_based = TRUE)

# Prep the recipe
trees_prep <- prep(tree_recipe)

# Bake the recipe to extract a preprocessed tree data
baked_trees <- bake(trees_prep, new_data = NULL)
```

### Data Splitting

> Question 2: Create trees_training (70%) and trees_test (30%) splits for the modeling

```{r}
set.seed(375)
#Split the data using th einitial_split function with a proportion = 0.7 to select 70% of the data for training and 30%for tetsing
tree_split <- initial_split(baked_trees, prop = 0.7)

#Define the trining data
tree_train <- training(tree_split)
#Define the testing data
tree_test <- testing(tree_split)
```

> Question 3: How many observations are we using for training with this split?
>
> We will be using 25,246 observation for this training set.

```{r}
length(tree_train$YrFireName)
```

### Simple Logistic Regression

Let's start our modeling effort with some simple models: one predictor and one outcome each.

> Question 4: Choose the three predictors that most highly correlate with our outcome variable for further investigation.

```{r}
#Create the correlation matrix
cor_matrix <- cor(baked_trees)
# Make a correlation plot between the variables
corrplot(cor_matrix, method = "shade", shade.col = NA, tl.col = "black", tl.srt = 45, addCoef.col = "black", cl.pos = "n", order = "original")
```

> Question 5: Use glm() to fit three simple logistic regression models, one for each of the predictors you identified.

```{r}
#Run the logistic regression for the crown volume scortch percentage with a correlation of 0.68
crown_vol_scorch_percent <- glm(data = tree_train, yr1status ~ CVS_percent, family = "binomial")

#Run the logistic regression for the bark char maximum witha correlation of 0.42
max_bark_char <- glm(data = tree_train, yr1status ~ BCHM_m, family = "binomial")

#Run the logistic regression for the tree diameter with a correlation of -0.32
tree_diam <- glm(data = tree_train, yr1status ~ DBH_cm, family = "binomial")

```

### Interpret the Coefficients

We aren't always interested in or able to interpret the model coefficients in a machine learning task. Often predictive accuracy is all we care about. odds when everything equals zero

```{r}
set.seed(375)
exp(coef(crown_vol_scorch_percent))
exp(coef(max_bark_char))
exp(coef(tree_diam))
```

> Question 6: That said, take a stab at interpreting our model coefficients now.

The odds of a tree dying increases multiplicatively by 1.08 for every one percentage point increase in the crown volume scorched.

The odds of a tree dying increases multiplicatively by 1.006 for every one meter increase in the maximum bark char height.

The odds of a tree dying increases multiplicatively by 0.996 for every one one centimeter increase in the base tree diameter.

> Question 7: Now let's visualize the results from these models. Plot the fit to the training data of each model.

```{r}
ggplot(tree_train, aes(x=CVS_percent, y= yr1status)) +
  geom_point()+
  stat_smooth(method="glm", se=FALSE, method.args = list(family= binomial))

ggplot(tree_train, aes(x=BCHM_m, y= yr1status)) +
  geom_point()+
  stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial))

ggplot(tree_train, aes(x=DBH_cm, y= yr1status)) +
  geom_point()+
  stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial))
```

### Multiple Logistic Regression

Let's not limit ourselves to a single-predictor model. More predictors might lead to better model performance.

> Question 8: Use glm() to fit a multiple logistic regression called "logistic_full", with all three of the predictors included. Which of these are significant in the resulting model?
>
> According to the summary statistics for the "logistic full" regression, all the predictors are significant. This is because each of the estimates have a p-value that is basically zero.

```{r}
logistic_full <- glm(yr1status ~ CVS_percent + BCHM_m + DBH_cm, family = "binomial", data = tree_train)
summary(logistic_full)
```

### Estimate Model Accuracy

Now we want to estimate our model's generalizability using resampling.

> Question 9: Use cross validation to assess model accuracy. Use caret::train() to fit four 10-fold cross-validated models (cv_model1, cv_model2, cv_model3, cv_model4) that correspond to each of the four models we've fit so far: three simple logistic regression models corresponding to each of the three key predictors (CVS_percent, DBH_cm, BCHM_m) and a multiple logistic regression model that combines all three predictors.

```{r}
#Set the outcome variable equal to a facot so it can be intrepreted more easily by the cross-validation models
fct_train <- as_factor(tree_train$yr1status)
tree_train <- tree_train %>% 
  mutate(fct_outcome = fct_train)

#Single prdictor cross validation model
cv_model1 <- train(
  fct_outcome ~ CVS_percent, 
  data = tree_train, 
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)
#Single prdictor cross validation model
cv_model2 <- train(
  fct_outcome ~ BCHM_m, 
  data = tree_train, 
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)
#Single prdictor cross validation model
cv_model3 <- train(
  fct_outcome ~ DBH_cm, 
  data = tree_train, 
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)
#Multiple prdictor cross validation model
cv_model4 <- train(
  fct_outcome ~ CVS_percent + BCHM_m + DBH_cm, 
  data = tree_train, 
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)
```

> Question 10: Use caret::resamples() to extract then compare the classification accuracy for each model. (Hint: resamples() wont give you what you need unless you convert the outcome variable to factor form). Which model has the highest accuracy?
>
> The multiple predictor model is the most accurate of the four models, we can determine this by comparing the accuracy statistics from the table produced below.

```{r}
set.seed(375)
#Create a summary table that prints the accuracy statistics of each of the cross validation models.
summary(
  resamples(
    list(
      model1 = cv_model1, 
      model2 = cv_model2, 
      model3 = cv_model3,
      model4 = cv_model4
    )
  )
)$statistics$Accuracy
```

Let's move forward with this single most accurate model.

> Question 11: Compute the confusion matrix and overall fraction of correct predictions by the model.

```{r}
set.seed(375)
# predict class
pred_class <- predict(cv_model4, tree_train)

# create confusion matrix
confusionMatrix(
  data = relevel(pred_class, ref = "1"), 
  reference = relevel(tree_train$fct_outcome, ref = "1")
)
```

> Question 12: Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.
>
> The overall accuracy of the model is roughly 90%, which is the fraction of correct predictions made by the model. Based of the confusion matrix above, we can see that there were 1574 Type 1 errors and 800 Type 2 errors. The False Positive Rate for this model is roughly 20% and the False negative rate for this model is roughly 4%. This makes sense because there were so many more observations of zeros than ones in this dataset. From the confusion matrix we can see that there were also 6240 true positive observations, and 16632 true negative observations.

> Question 13: What is the overall accuracy of the model? How is this calculated?

The overall accuracy of the model is \~90%, this is calculated using the following equation: (TP + TN) / (TP+TN+FN+FP)

### Test Final Model

Alright, now we'll take our most accurate model and make predictions on some unseen data (the test data).

> Question 14: Now that we have identified our best model, evaluate it by running a prediction on the test data, trees_test.

```{r}
set.seed(375)
fct_test <- as_factor(tree_test$yr1status)
tree_test <- tree_test %>% 
  mutate(fct_outcome = fct_test)

# predict class
pred_class_test <- predict(cv_model4, tree_test)

# create confusion matrix
confusionMatrix(
  data = relevel(pred_class_test, ref = "1"), 
  reference = relevel(tree_test$fct_outcome, ref = "1")
)
```

> Question 15: How does the accuracy of this final model on the test data compare to its cross validation accuracy? Do you find this to be surprising? Why or why not?
>
> The accuracy on the testing data is slightly lower than the accuracy from the training dataset, but is overall almost the same. This is not surprising to me because I would expect the model to perform slightly better at predicting the data it was trained on. However the accuracies should be very similar because the training dataset is so large, I imagine that are not very many data points in the testing data set that are super unique from the points in the training data set. I would generally expect a model to perform better on the training data than on the unseen testing data.
